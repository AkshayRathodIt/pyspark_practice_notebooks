{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/akshay/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark_tutorials_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data = [\n",
    "    (1, 'John Doe', 1001, 50000),\n",
    "    (2, 'Jane Smith', 1002, 60000),\n",
    "    (3, 'Michael Johnson', 1001, 55000),\n",
    "    (4, 'Emily Davis', 1003, 65000),\n",
    "    (5, 'David Brown', 1002, 62000),\n",
    "    (6, 'Sarah Clark', 1003, 68000),\n",
    "    (7, 'Kevin Lee', 1001, 51000),\n",
    "    (8, 'Rachel Wilson', 1002, 59000),\n",
    "    (9, 'Jason Martinez', 1001, 54000),\n",
    "    (10, 'Laura Garcia', 1003, 70000),\n",
    "    (11, 'Daniel Anderson', 1002, 63000),\n",
    "    (12, 'Maria Rodriguez', 1003, 69000),\n",
    "    (13, 'Eric Wright', 1001, 52000),\n",
    "    (14, 'Olivia Thomas', 1003, 71000),\n",
    "    (15, 'Andrew Scott', 1002, 60000),\n",
    "    (16, 'Sophia Allen', 1004, 58000),   # New employee\n",
    "    (17, 'Thomas Baker', 1001, 56000),   # New employee\n",
    "    (18, 'Jessica White', 1004, 59000),  # New employee\n",
    "    (19, 'Ryan King', 1002, 64000),      # New employee\n",
    "    (20, 'Emma Young', 1004, 62000)  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = [\"_emp_id\",\"emp_name\",\"dept_id\",\"emp_salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(data=employee_data, schema=employee_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------+----------+\n",
      "|_emp_id|       emp_name|dept_id|emp_salary|\n",
      "+-------+---------------+-------+----------+\n",
      "|      1|       John Doe|   1001|     50000|\n",
      "|      2|     Jane Smith|   1002|     60000|\n",
      "|      3|Michael Johnson|   1001|     55000|\n",
      "|      4|    Emily Davis|   1003|     65000|\n",
      "|      5|    David Brown|   1002|     62000|\n",
      "|      6|    Sarah Clark|   1003|     68000|\n",
      "|      7|      Kevin Lee|   1001|     51000|\n",
      "|      8|  Rachel Wilson|   1002|     59000|\n",
      "|      9| Jason Martinez|   1001|     54000|\n",
      "|     10|   Laura Garcia|   1003|     70000|\n",
      "|     11|Daniel Anderson|   1002|     63000|\n",
      "|     12|Maria Rodriguez|   1003|     69000|\n",
      "|     13|    Eric Wright|   1001|     52000|\n",
      "|     14|  Olivia Thomas|   1003|     71000|\n",
      "|     15|   Andrew Scott|   1002|     60000|\n",
      "|     16|   Sophia Allen|   1004|     58000|\n",
      "|     17|   Thomas Baker|   1001|     56000|\n",
      "|     18|  Jessica White|   1004|     59000|\n",
      "|     19|      Ryan King|   1002|     64000|\n",
      "|     20|     Emma Young|   1004|     62000|\n",
      "+-------+---------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for departments\n",
    "department_data = [\n",
    "    (1001, 'HR'),\n",
    "    (1002, 'IT'),\n",
    "    (1003, 'Finance'),\n",
    "    (1005, \"sales\"),\n",
    "    (1006, \"Marketing\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_schema = [\"dept_id\",\"dept_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_id|dept_name|\n",
      "+-------+---------+\n",
      "|   1001|       HR|\n",
      "|   1002|       IT|\n",
      "|   1003|  Finance|\n",
      "|   1005|    sales|\n",
      "|   1006|Marketing|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df = spark.createDataFrame(data=department_data, schema=dept_schema)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        Joined DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      "    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "    >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      "    >>> df4 = spark.createDataFrame([\n",
      "    ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "    ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "    ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "    ...     Row(age=None, height=None, name=None),\n",
      "    ... ])\n",
      "    \n",
      "    Inner join on columns (default)\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      "    +----+------+\n",
      "    |name|height|\n",
      "    +----+------+\n",
      "    | Bob|    85|\n",
      "    +----+------+\n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    | Bob|  5|\n",
      "    +----+---+\n",
      "    \n",
      "    Outer join for both DataFrames on the 'name' column.\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      "    ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Bob|    85|\n",
      "    |Alice|  NULL|\n",
      "    | NULL|    80|\n",
      "    +-----+------+\n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Tom|    80|\n",
      "    |  Bob|    85|\n",
      "    |Alice|  NULL|\n",
      "    +-----+------+\n",
      "    \n",
      "    Outer join for both DataFrams with multiple columns.\n",
      "    \n",
      "    >>> df.join(\n",
      "    ...     df3,\n",
      "    ...     [df.name == df3.name, df.age == df3.age],\n",
      "    ...     'outer'\n",
      "    ... ).select(df.name, df3.age).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  2|\n",
      "    |  Bob|  5|\n",
      "    +-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(emp_df.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|_emp_id|       emp_name|dept_id|emp_salary|dept_id|dept_name|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|      1|       John Doe|   1001|     50000|   1001|       HR|\n",
      "|      3|Michael Johnson|   1001|     55000|   1001|       HR|\n",
      "|      7|      Kevin Lee|   1001|     51000|   1001|       HR|\n",
      "|      9| Jason Martinez|   1001|     54000|   1001|       HR|\n",
      "|     13|    Eric Wright|   1001|     52000|   1001|       HR|\n",
      "|     17|   Thomas Baker|   1001|     56000|   1001|       HR|\n",
      "|      2|     Jane Smith|   1002|     60000|   1002|       IT|\n",
      "|      5|    David Brown|   1002|     62000|   1002|       IT|\n",
      "|      8|  Rachel Wilson|   1002|     59000|   1002|       IT|\n",
      "|     11|Daniel Anderson|   1002|     63000|   1002|       IT|\n",
      "|     15|   Andrew Scott|   1002|     60000|   1002|       IT|\n",
      "|     19|      Ryan King|   1002|     64000|   1002|       IT|\n",
      "|      4|    Emily Davis|   1003|     65000|   1003|  Finance|\n",
      "|      6|    Sarah Clark|   1003|     68000|   1003|  Finance|\n",
      "|     10|   Laura Garcia|   1003|     70000|   1003|  Finance|\n",
      "|     12|Maria Rodriguez|   1003|     69000|   1003|  Finance|\n",
      "|     14|  Olivia Thomas|   1003|     71000|   1003|  Finance|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inner_df = emp_df.join(dept_df, on=(emp_df.dept_id == dept_df.dept_id), how=\"inner\")\n",
    "inner_df.show()\n",
    "print(inner_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|_emp_id|       emp_name|dept_id|emp_salary|dept_id|dept_name|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|      2|     Jane Smith|   1002|     60000|   1002|       IT|\n",
      "|      5|    David Brown|   1002|     62000|   1002|       IT|\n",
      "|      1|       John Doe|   1001|     50000|   1001|       HR|\n",
      "|      3|Michael Johnson|   1001|     55000|   1001|       HR|\n",
      "|      4|    Emily Davis|   1003|     65000|   1003|  Finance|\n",
      "|      8|  Rachel Wilson|   1002|     59000|   1002|       IT|\n",
      "|      7|      Kevin Lee|   1001|     51000|   1001|       HR|\n",
      "|      9| Jason Martinez|   1001|     54000|   1001|       HR|\n",
      "|      6|    Sarah Clark|   1003|     68000|   1003|  Finance|\n",
      "|     10|   Laura Garcia|   1003|     70000|   1003|  Finance|\n",
      "|     11|Daniel Anderson|   1002|     63000|   1002|       IT|\n",
      "|     15|   Andrew Scott|   1002|     60000|   1002|       IT|\n",
      "|     13|    Eric Wright|   1001|     52000|   1001|       HR|\n",
      "|     12|Maria Rodriguez|   1003|     69000|   1003|  Finance|\n",
      "|     14|  Olivia Thomas|   1003|     71000|   1003|  Finance|\n",
      "|     19|      Ryan King|   1002|     64000|   1002|       IT|\n",
      "|     17|   Thomas Baker|   1001|     56000|   1001|       HR|\n",
      "|     16|   Sophia Allen|   1004|     58000|   NULL|     NULL|\n",
      "|     18|  Jessica White|   1004|     59000|   NULL|     NULL|\n",
      "|     20|     Emma Young|   1004|     62000|   NULL|     NULL|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "left_df = emp_df.join(dept_df, on=(emp_df.dept_id == dept_df.dept_id), how=\"left\")\n",
    "left_df.show()\n",
    "print(left_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|_emp_id|       emp_name|dept_id|emp_salary|dept_id|dept_name|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|     17|   Thomas Baker|   1001|     56000|   1001|       HR|\n",
      "|     13|    Eric Wright|   1001|     52000|   1001|       HR|\n",
      "|      9| Jason Martinez|   1001|     54000|   1001|       HR|\n",
      "|      7|      Kevin Lee|   1001|     51000|   1001|       HR|\n",
      "|      3|Michael Johnson|   1001|     55000|   1001|       HR|\n",
      "|      1|       John Doe|   1001|     50000|   1001|       HR|\n",
      "|     19|      Ryan King|   1002|     64000|   1002|       IT|\n",
      "|     15|   Andrew Scott|   1002|     60000|   1002|       IT|\n",
      "|     11|Daniel Anderson|   1002|     63000|   1002|       IT|\n",
      "|      8|  Rachel Wilson|   1002|     59000|   1002|       IT|\n",
      "|      5|    David Brown|   1002|     62000|   1002|       IT|\n",
      "|      2|     Jane Smith|   1002|     60000|   1002|       IT|\n",
      "|     14|  Olivia Thomas|   1003|     71000|   1003|  Finance|\n",
      "|     12|Maria Rodriguez|   1003|     69000|   1003|  Finance|\n",
      "|     10|   Laura Garcia|   1003|     70000|   1003|  Finance|\n",
      "|      6|    Sarah Clark|   1003|     68000|   1003|  Finance|\n",
      "|      4|    Emily Davis|   1003|     65000|   1003|  Finance|\n",
      "|   NULL|           NULL|   NULL|      NULL|   1005|    sales|\n",
      "|   NULL|           NULL|   NULL|      NULL|   1006|Marketing|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "right_df = emp_df.join(dept_df, on=(emp_df.dept_id == dept_df.dept_id), how=\"right\")\n",
    "right_df.show()\n",
    "print(right_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|_emp_id|       emp_name|dept_id|emp_salary|dept_id|dept_name|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "|      1|       John Doe|   1001|     50000|   1001|       HR|\n",
      "|      3|Michael Johnson|   1001|     55000|   1001|       HR|\n",
      "|      7|      Kevin Lee|   1001|     51000|   1001|       HR|\n",
      "|      9| Jason Martinez|   1001|     54000|   1001|       HR|\n",
      "|     13|    Eric Wright|   1001|     52000|   1001|       HR|\n",
      "|     17|   Thomas Baker|   1001|     56000|   1001|       HR|\n",
      "|      2|     Jane Smith|   1002|     60000|   1002|       IT|\n",
      "|      5|    David Brown|   1002|     62000|   1002|       IT|\n",
      "|      8|  Rachel Wilson|   1002|     59000|   1002|       IT|\n",
      "|     11|Daniel Anderson|   1002|     63000|   1002|       IT|\n",
      "|     15|   Andrew Scott|   1002|     60000|   1002|       IT|\n",
      "|     19|      Ryan King|   1002|     64000|   1002|       IT|\n",
      "|      4|    Emily Davis|   1003|     65000|   1003|  Finance|\n",
      "|      6|    Sarah Clark|   1003|     68000|   1003|  Finance|\n",
      "|     10|   Laura Garcia|   1003|     70000|   1003|  Finance|\n",
      "|     12|Maria Rodriguez|   1003|     69000|   1003|  Finance|\n",
      "|     14|  Olivia Thomas|   1003|     71000|   1003|  Finance|\n",
      "|     16|   Sophia Allen|   1004|     58000|   NULL|     NULL|\n",
      "|     18|  Jessica White|   1004|     59000|   NULL|     NULL|\n",
      "|     20|     Emma Young|   1004|     62000|   NULL|     NULL|\n",
      "|   NULL|           NULL|   NULL|      NULL|   1005|    sales|\n",
      "|   NULL|           NULL|   NULL|      NULL|   1006|Marketing|\n",
      "+-------+---------------+-------+----------+-------+---------+\n",
      "\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_df = emp_df.join(dept_df, on=(emp_df.dept_id == dept_df.dept_id), how=\"outer\")\n",
    "full_df.show(n=22)\n",
    "print(full_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
