{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d946e92d-edf8-4e08-8487-000f6087d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34805664-16ad-44ce-a9a4-fb32eca1d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/akshay/spark-3.5.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d2cedd3-f731-4b1c-9873-60f9a8fcb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541e9487-ddde-4e78-a586-6329f81a4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark_tutorials\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113b583-9ce2-428b-9329-e3e88ac8b677",
   "metadata": {},
   "source": [
    "### create a dataframe manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cbf3b7-1c4a-4796-a21a-a2a26ea2e337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f0096c-f58c-4224-8cbb-c5816f39cb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'active',\n",
       " 'addArtifact',\n",
       " 'addArtifacts',\n",
       " 'addTag',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'clearTags',\n",
       " 'client',\n",
       " 'conf',\n",
       " 'copyFromLocalToFs',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'getTags',\n",
       " 'interruptAll',\n",
       " 'interruptOperation',\n",
       " 'interruptTag',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'removeTag',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'udtf',\n",
       " 'version']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1f8110c-7e64-450a-9a6a-bcffe47e7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "    or a :class:`numpy.ndarray`.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None. The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    \n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "    \n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "        later.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring. The first few rows will be used\n",
      "        if ``samplingRatio`` is ``None``.\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "        .. versionadded:: 2.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create a DataFrame from a list of tuples.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
      "    +-----+---+\n",
      "    |   _1| _2|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a list of dictionaries.\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  1|Alice|\n",
      "    +---+-----+\n",
      "    \n",
      "    Create a DataFrame with column names specified.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the explicit schema specified.\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the schema in DDL formatted string.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create an empty DataFrame.\n",
      "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
      "    as the DataFrame lacks data from which the schema can be inferred.\n",
      "    \n",
      "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    +----+---+\n",
      "    \n",
      "    Create a DataFrame from Row objects.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
      "    >>> df.show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a pandas DataFrame.\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    +---+---+\n",
      "    |  0|  1|\n",
      "    +---+---+\n",
      "    |  1|  2|\n",
      "    +---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76190d80-22a1-4b4e-9a16-4b2ac241bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,\"Akshay\"),(2,\"Harshada\"),(3,\"Mrunali\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b51d918-70b1-4146-9198-9165e0fc3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe9138a8-e7d1-43c9-9c16-3397008846ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| _1|      _2|\n",
      "+---+--------+\n",
      "|  1|  Akshay|\n",
      "|  2|Harshada|\n",
      "|  3| Mrunali|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "913ec64a-4b9d-41aa-ab5b-49e51b8a3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f3dbfdd-8e5d-452c-afe5-af6f389155bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Akshay|\n",
      "|  2|Harshada|\n",
      "|  3| Mrunali|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b9f4529-af46-48b3-9016-a01445c49bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3457b260-7e95-4136-867b-1fed2d960913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac2b7a50-20ed-45a2-8442-29e6e10a075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StructType in module pyspark.sql.types:\n",
      "\n",
      "class StructType(DataType)\n",
      " |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
      " |  \n",
      " |  Struct type, consisting of a list of :class:`StructField`.\n",
      " |  \n",
      " |  This is the data type representing a :class:`Row`.\n",
      " |  \n",
      " |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n",
      " |  A contained :class:`StructField` can be accessed by its name or position.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.sql.types import *\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1[\"f1\"]\n",
      " |  StructField('f1', StringType(), True)\n",
      " |  >>> struct1[0]\n",
      " |  StructField('f1', StringType(), True)\n",
      " |  \n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n",
      " |  >>> struct1 == struct2\n",
      " |  True\n",
      " |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      " |  ...     StructField(\"f2\", IntegerType(), False)])\n",
      " |  >>> struct1 == struct2\n",
      " |  False\n",
      " |  \n",
      " |  The below example demonstrates how to create a DataFrame based on a struct created\n",
      " |  using class:`StructType` and class:`StructField`:\n",
      " |  \n",
      " |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n",
      " |  >>> schema = StructType([\n",
      " |  ...     StructField(\"name\", StringType()),\n",
      " |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n",
      " |  ... ])\n",
      " |  >>> df = spark.createDataFrame(data=data, schema=schema)\n",
      " |  >>> df.printSchema()\n",
      " |  root\n",
      " |   |-- name: string (nullable = true)\n",
      " |   |-- languagesSkills: array (nullable = true)\n",
      " |   |    |-- element: string (containsNull = true)\n",
      " |  >>> df.show()\n",
      " |  +-----+---------------+\n",
      " |  | name|languagesSkills|\n",
      " |  +-----+---------------+\n",
      " |  |Alice|  [Java, Scala]|\n",
      " |  |  Bob|[Python, Scala]|\n",
      " |  +-----+---------------+\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StructType\n",
      " |      DataType\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n",
      " |      Access fields by name or slice.\n",
      " |  \n",
      " |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n",
      " |      Iterate the fields\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Return the number of fields.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n",
      " |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n",
      " |      The method accepts either:\n",
      " |      \n",
      " |          a) A single parameter which is a :class:`StructField` object.\n",
      " |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
      " |             metadata(optional). The data_type parameter may be either a String or a\n",
      " |             :class:`DataType` object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field : str or :class:`StructField`\n",
      " |          Either the name of the field or a :class:`StructField` object\n",
      " |      data_type : :class:`DataType`, optional\n",
      " |          If present, the DataType of the :class:`StructField` to create\n",
      " |      nullable : bool, optional\n",
      " |          Whether the field to add should be nullable (default True)\n",
      " |      metadata : dict, optional\n",
      " |          Any additional metadata (default None)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
      " |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      " |      ...     StructField(\"f2\", StringType(), True, None)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
      " |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct1 == struct2\n",
      " |      True\n",
      " |  \n",
      " |  fieldNames(self) -> List[str]\n",
      " |      Returns all field names in a list.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.types import StringType, StructField, StructType\n",
      " |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
      " |      >>> struct.fieldNames()\n",
      " |      ['f1']\n",
      " |  \n",
      " |  fromInternal(self, obj: Tuple) -> 'Row'\n",
      " |      Converts an internal SQL object into a native Python object.\n",
      " |  \n",
      " |  jsonValue(self) -> Dict[str, Any]\n",
      " |  \n",
      " |  needConversion(self) -> bool\n",
      " |      Does this type needs conversion between Python object and internal SQL object.\n",
      " |      \n",
      " |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      " |  \n",
      " |  simpleString(self) -> str\n",
      " |  \n",
      " |  toInternal(self, obj: Tuple) -> Tuple\n",
      " |      Converts a Python object into an internal SQL object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n",
      " |      Constructs :class:`StructType` from a schema defined in JSON format.\n",
      " |      \n",
      " |      Below is a JSON schema it must adhere to::\n",
      " |      \n",
      " |          {\n",
      " |            \"title\":\"StructType\",\n",
      " |            \"description\":\"Schema of StructType in json format\",\n",
      " |            \"type\":\"object\",\n",
      " |            \"properties\":{\n",
      " |               \"fields\":{\n",
      " |                  \"description\":\"Array of struct fields\",\n",
      " |                  \"type\":\"array\",\n",
      " |                  \"items\":{\n",
      " |                      \"type\":\"object\",\n",
      " |                      \"properties\":{\n",
      " |                         \"name\":{\n",
      " |                            \"description\":\"Name of the field\",\n",
      " |                            \"type\":\"string\"\n",
      " |                         },\n",
      " |                         \"type\":{\n",
      " |                            \"description\": \"Type of the field. Can either be\n",
      " |                                            another nested StructType or primitive type\",\n",
      " |                            \"type\":\"object/string\"\n",
      " |                         },\n",
      " |                         \"nullable\":{\n",
      " |                            \"description\":\"If nulls are allowed\",\n",
      " |                            \"type\":\"boolean\"\n",
      " |                         },\n",
      " |                         \"metadata\":{\n",
      " |                            \"description\":\"Additional metadata to supply\",\n",
      " |                            \"type\":\"object\"\n",
      " |                         },\n",
      " |                         \"required\":[\n",
      " |                            \"name\",\n",
      " |                            \"type\",\n",
      " |                            \"nullable\",\n",
      " |                            \"metadata\"\n",
      " |                         ]\n",
      " |                      }\n",
      " |                 }\n",
      " |              }\n",
      " |           }\n",
      " |         }\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      json : dict or a dict-like object e.g. JSON object\n",
      " |          This \"dict\" must have \"fields\" key that returns an array of fields\n",
      " |          each of which must have specific keys (name, type, nullable, metadata).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> json_str = '''\n",
      " |      ...  {\n",
      " |      ...      \"fields\": [\n",
      " |      ...          {\n",
      " |      ...              \"metadata\": {},\n",
      " |      ...              \"name\": \"Person\",\n",
      " |      ...              \"nullable\": true,\n",
      " |      ...              \"type\": {\n",
      " |      ...                  \"fields\": [\n",
      " |      ...                      {\n",
      " |      ...                          \"metadata\": {},\n",
      " |      ...                          \"name\": \"name\",\n",
      " |      ...                          \"nullable\": false,\n",
      " |      ...                          \"type\": \"string\"\n",
      " |      ...                      },\n",
      " |      ...                      {\n",
      " |      ...                          \"metadata\": {},\n",
      " |      ...                          \"name\": \"surname\",\n",
      " |      ...                          \"nullable\": false,\n",
      " |      ...                          \"type\": \"string\"\n",
      " |      ...                      }\n",
      " |      ...                  ],\n",
      " |      ...                  \"type\": \"struct\"\n",
      " |      ...              }\n",
      " |      ...          }\n",
      " |      ...      ],\n",
      " |      ...      \"type\": \"struct\"\n",
      " |      ...  }\n",
      " |      ...  '''\n",
      " |      >>> import json\n",
      " |      >>> scheme = StructType.fromJson(json.loads(json_str))\n",
      " |      >>> scheme.simpleString()\n",
      " |      'struct<Person:struct<name:string,surname:string>>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DataType:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self) -> int\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __ne__(self, other: Any) -> bool\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  json(self) -> str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from DataType:\n",
      " |  \n",
      " |  typeName() -> str from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DataType:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a82884-6b09-480e-a600-c624829aac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"id\",IntegerType(),True),\n",
    "                   StructField(\"name\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0684160e-41a8-4edb-af8d-ab9a9dad416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "043ba0d8-fa95-49a8-96df-db25bf20966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Akshay|\n",
      "|  2|Harshada|\n",
      "|  3| Mrunali|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "140d7998-856b-4df3-81dc-5663d33e6af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a9a98-9385-4dbc-905c-2be91db878b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
