{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/27 19:20:45 WARN Utils: Your hostname, akshay-vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/06/27 19:20:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/27 19:20:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/akshay/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark_tutorials_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding `partitionBy` in Apache Spark\n",
    "\n",
    "In Apache Spark, `partitionBy` is a method used on DataFrames or Datasets to partition data based on one or more columns. Partitioning is a crucial optimization technique in distributed data processing that improves query performance, especially when dealing with large datasets.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Partitioning:** Divides data into groups or partitions based on a specified column or expression. Each partition is processed independently by different nodes (executors) in the Spark cluster.\n",
    "\n",
    "2. **Usage:**\n",
    "   - **Data Organization:** `partitionBy` is typically used when writing data to disk or another storage system (like HDFS or Amazon S3). It allows data to be organized into directories or files based on the partitioning column, facilitating efficient data retrieval and processing.\n",
    "   \n",
    "   - **Performance:** Partitioning can significantly enhance performance by minimizing data shuffling during joins, aggregations, and other operations. It ensures that related data is co-located within the same partition, reducing the amount of data movement across the cluster.\n",
    "\n",
    "3. **Syntax:**\n",
    "   ```scala\n",
    "   // Example of using partitionBy in Scala\n",
    "   val df = spark.read.format(\"csv\").load(\"path/to/data\")\n",
    "   df.write.partitionBy(\"partition_column\").format(\"parquet\").save(\"output_path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"Female\", \"IT\"),\n",
    "    (2, \"Bob\", \"Male\", \"Finance\"),\n",
    "    (3, \"Charlie\", \"Male\", \"HR\"),\n",
    "    (4, \"Diana\", \"Female\", \"IT\"),\n",
    "    (5, \"Eve\", \"Female\", \"Finance\"),\n",
    "    (6, \"Frank\", \"Male\", \"HR\"),\n",
    "    (7, \"Grace\", \"Female\", \"IT\"),\n",
    "    (8, \"Henry\", \"Male\", \"Finance\"),\n",
    "    (9, \"Ivy\", \"Female\", \"HR\"),\n",
    "    (10, \"Jack\", \"Male\", \"IT\"),\n",
    "    (11, \"Katherine\", \"Female\", \"Finance\"),\n",
    "    (12, \"Leo\", \"Male\", \"HR\"),\n",
    "    (13, \"Mary\", \"Female\", \"IT\"),\n",
    "    (14, \"Nick\", \"Male\", \"Finance\"),\n",
    "    (15, \"Olivia\", \"Female\", \"HR\"),\n",
    "    (16, \"Peter\", \"Male\", \"IT\"),\n",
    "    (17, \"Quinn\", \"Female\", \"Finance\"),\n",
    "    (18, \"Ryan\", \"Male\", \"HR\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"id\",dataType=IntegerType()), \\\n",
    "                        StructField(\"name\",dataType=StringType()), \\\n",
    "                        StructField(\"gender\",dataType=StringType()), \\\n",
    "                        StructField(\"dept\",dataType=StringType()) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+-------+\n",
      "| id|     name|gender|   dept|\n",
      "+---+---------+------+-------+\n",
      "|  1|    Alice|Female|     IT|\n",
      "|  2|      Bob|  Male|Finance|\n",
      "|  3|  Charlie|  Male|     HR|\n",
      "|  4|    Diana|Female|     IT|\n",
      "|  5|      Eve|Female|Finance|\n",
      "|  6|    Frank|  Male|     HR|\n",
      "|  7|    Grace|Female|     IT|\n",
      "|  8|    Henry|  Male|Finance|\n",
      "|  9|      Ivy|Female|     HR|\n",
      "| 10|     Jack|  Male|     IT|\n",
      "| 11|Katherine|Female|Finance|\n",
      "| 12|      Leo|  Male|     HR|\n",
      "| 13|     Mary|Female|     IT|\n",
      "| 14|     Nick|  Male|Finance|\n",
      "| 15|   Olivia|Female|     HR|\n",
      "| 16|    Peter|  Male|     IT|\n",
      "| 17|    Quinn|Female|Finance|\n",
      "| 18|     Ryan|  Male|     HR|\n",
      "+---+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(path=\"./data/employee_1/\", mode=\"overwrite\", partitionBy=\"dept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(path=\"./data/employee_2/\", mode=\"overwrite\", partitionBy=[\"dept\",\"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|gender|\n",
      "+---+-------+------+\n",
      "|  3|Charlie|  Male|\n",
      "| 15| Olivia|Female|\n",
      "|  6|  Frank|  Male|\n",
      "| 18|   Ryan|  Male|\n",
      "| 12|    Leo|  Male|\n",
      "|  9|    Ivy|Female|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1 = spark.read.parquet(\"./data/employee_2/dept=HR/\", schema=schema)\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
