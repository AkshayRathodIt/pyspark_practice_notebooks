{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 23:46:06 WARN Utils: Your hostname, akshay-vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/06/23 23:46:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 23:46:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/akshay/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark_tutorials_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-Defined Functions\n",
    "\n",
    "## Overview\n",
    "\n",
    "User-defined functions (UDFs) are functions that are created by users to perform specific tasks or computations within a program or script. They allow for code modularity, reusability, and abstraction of complex operations into simpler, named units.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Modularity**: UDFs break down complex tasks into smaller, manageable parts.\n",
    "- **Reusability**: Once defined, UDFs can be used multiple times within the same program or in different programs.\n",
    "- **Abstraction**: UDFs hide the implementation details of a task, making the code easier to understand and maintain.\n",
    "\n",
    "## Syntax\n",
    "\n",
    "In most programming languages, defining a UDF follows a common syntax:\n",
    "\n",
    "```python\n",
    "# Example in Python\n",
    "def function_name(parameters):\n",
    "    # Function body\n",
    "    # Perform operations\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    (1, 'John Doe', 50000, 2000),\n",
    "    (2, 'Jane Smith', 60000, 2500),\n",
    "    (3, 'Michael Johnson', 55000, 2200),\n",
    "    (4, 'Emily Davis', 52000, 1800),\n",
    "    (5, 'Chris Brown', 58000, 2700),\n",
    "    (6, 'Sarah Wilson', 53000, 2000),\n",
    "    (7, 'Kevin Lee', 51000, 1900),\n",
    "    (8, 'Jessica Moore', 59000, 2600),\n",
    "    (9, 'David Anderson', 54000, 2100),\n",
    "    (10, 'Amy Taylor', 57000, 2400)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\"id\", \"name\", \"salary\", \"bonas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------+-----+\n",
      "| id|           name|salary|bonas|\n",
      "+---+---------------+------+-----+\n",
      "|  1|       John Doe| 50000| 2000|\n",
      "|  2|     Jane Smith| 60000| 2500|\n",
      "|  3|Michael Johnson| 55000| 2200|\n",
      "|  4|    Emily Davis| 52000| 1800|\n",
      "|  5|    Chris Brown| 58000| 2700|\n",
      "|  6|   Sarah Wilson| 53000| 2000|\n",
      "|  7|      Kevin Lee| 51000| 1900|\n",
      "|  8|  Jessica Moore| 59000| 2600|\n",
      "|  9| David Anderson| 54000| 2100|\n",
      "| 10|     Amy Taylor| 57000| 2400|\n",
      "+---+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=employees, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets create custom function for totalsalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalSalary(s,b):\n",
    "    return s+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function udf in module pyspark.sql.functions:\n",
      "\n",
      "udf(f: Union[Callable[..., Any], ForwardRef('DataTypeOrString'), NoneType] = None, returnType: 'DataTypeOrString' = StringType(), *, useArrow: Optional[bool] = None) -> Union[ForwardRef('UserDefinedFunctionLike'), Callable[[Callable[..., Any]], ForwardRef('UserDefinedFunctionLike')]]\n",
      "    Creates a user defined function (UDF).\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    f : function\n",
      "        python function if used as a standalone function\n",
      "    returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "        the return type of the user-defined function. The value can be either a\n",
      "        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "    useArrow : bool or None\n",
      "        whether to use Arrow to optimize the (de)serialization. When it is None, the\n",
      "        Spark config \"spark.sql.execution.pythonUDF.arrow.enabled\" takes effect.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "    >>> @udf\n",
      "    ... def to_upper(s):\n",
      "    ...     if s is not None:\n",
      "    ...         return s.upper()\n",
      "    ...\n",
      "    >>> @udf(returnType=IntegerType())\n",
      "    ... def add_one(x):\n",
      "    ...     if x is not None:\n",
      "    ...         return x + 1\n",
      "    ...\n",
      "    >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "    >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "    +----------+--------------+------------+\n",
      "    |slen(name)|to_upper(name)|add_one(age)|\n",
      "    +----------+--------------+------------+\n",
      "    |         8|      JOHN DOE|          22|\n",
      "    +----------+--------------+------------+\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The user-defined functions are considered deterministic by default. Due to\n",
      "    optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "    more times than it is present in the query. If your function is not deterministic, call\n",
      "    `asNondeterministic` on the user defined function. E.g.:\n",
      "    \n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> import random\n",
      "    >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "    \n",
      "    The user-defined functions do not support conditional expressions or short circuiting\n",
      "    in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "    can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "    \n",
      "    The user-defined functions do not take keyword arguments on the calling side.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalPayment = udf(lambda s,b : totalSalary(s,b), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------+-----+------------+\n",
      "| id|           name|salary|bonas|totalPayment|\n",
      "+---+---------------+------+-----+------------+\n",
      "|  1|       John Doe| 50000| 2000|       52000|\n",
      "|  2|     Jane Smith| 60000| 2500|       62500|\n",
      "|  3|Michael Johnson| 55000| 2200|       57200|\n",
      "|  4|    Emily Davis| 52000| 1800|       53800|\n",
      "|  5|    Chris Brown| 58000| 2700|       60700|\n",
      "|  6|   Sarah Wilson| 53000| 2000|       55000|\n",
      "|  7|      Kevin Lee| 51000| 1900|       52900|\n",
      "|  8|  Jessica Moore| 59000| 2600|       61600|\n",
      "|  9| David Anderson| 54000| 2100|       56100|\n",
      "| 10|     Amy Taylor| 57000| 2400|       59400|\n",
      "+---+---------------+------+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"totalPayment\", TotalPayment(df.salary, df.bonas))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can register the function as below as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def totalSalary(s,b):\n",
    "    return s+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------+-----+------------+\n",
      "| id|           name|salary|bonas|total_salary|\n",
      "+---+---------------+------+-----+------------+\n",
      "|  1|       John Doe| 50000| 2000|       52000|\n",
      "|  2|     Jane Smith| 60000| 2500|       62500|\n",
      "|  3|Michael Johnson| 55000| 2200|       57200|\n",
      "|  4|    Emily Davis| 52000| 1800|       53800|\n",
      "|  5|    Chris Brown| 58000| 2700|       60700|\n",
      "|  6|   Sarah Wilson| 53000| 2000|       55000|\n",
      "|  7|      Kevin Lee| 51000| 1900|       52900|\n",
      "|  8|  Jessica Moore| 59000| 2600|       61600|\n",
      "|  9| David Anderson| 54000| 2100|       56100|\n",
      "| 10|     Amy Taylor| 57000| 2400|       59400|\n",
      "+---+---------------+------+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"*\", totalSalary(df.salary, df.bonas).alias(\"total_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalPay(s,b):\n",
    "    return s+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.totalPay(s, b)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"totalPaySQL\",totalPay, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"SELECT *, totalPaySQL('salary','bonas') as totalpay FROM emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------+-----+--------+\n",
      "| id|           name|salary|bonas|totalpay|\n",
      "+---+---------------+------+-----+--------+\n",
      "|  1|       John Doe| 50000| 2000|    NULL|\n",
      "|  2|     Jane Smith| 60000| 2500|    NULL|\n",
      "|  3|Michael Johnson| 55000| 2200|    NULL|\n",
      "|  4|    Emily Davis| 52000| 1800|    NULL|\n",
      "|  5|    Chris Brown| 58000| 2700|    NULL|\n",
      "|  6|   Sarah Wilson| 53000| 2000|    NULL|\n",
      "|  7|      Kevin Lee| 51000| 1900|    NULL|\n",
      "|  8|  Jessica Moore| 59000| 2600|    NULL|\n",
      "|  9| David Anderson| 54000| 2100|    NULL|\n",
      "| 10|     Amy Taylor| 57000| 2400|    NULL|\n",
      "+---+---------------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
