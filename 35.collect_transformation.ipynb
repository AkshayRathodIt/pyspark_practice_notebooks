{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 21:28:26 WARN Utils: Your hostname, akshay-vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/06/23 21:28:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 21:28:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/23 21:28:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/akshay/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark_tutorials_1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark `collect` Function: Overview, Pros, and Cons\n",
    "\n",
    "The `collect` function in Apache Spark is used to retrieve all the elements of a distributed dataset (RDD or DataFrame) to the driver program. It returns the data as a local collection (e.g., Python list or NumPy array), which can be processed using standard Python operations.\n",
    "\n",
    "### Syntax\n",
    "\n",
    "```python\n",
    "# For RDDs:\n",
    "data = rdd.collect()\n",
    "\n",
    "# For DataFrames:\n",
    "data = df.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark `collect` Function: Pros, Cons, and Best Practices\n",
    "\n",
    "The `collect` function in Apache Spark is used to retrieve all the elements of a distributed dataset (RDD or DataFrame) to the driver program. It returns the data as a local collection (e.g., Python list or NumPy array), which can be processed using standard Python operations.\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Local Processing**: Data collected using `collect` is available locally in the driver program. This allows for easy integration with local Python libraries and facilitates further analysis or processing.\n",
    "\n",
    "- **Ease of Use**: Once collected, the data can be manipulated using familiar Python operations, making it straightforward to perform subsequent calculations or transformations.\n",
    "\n",
    "- **Debugging**: Useful for debugging and exploring small datasets extracted from larger distributed datasets. It enables quick inspection of data contents and structure.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Memory Intensive**: The `collect` function brings all data to the driver program, which can lead to memory issues if the dataset is too large to fit in memory on the driver. This limits its usefulness for very large datasets.\n",
    "\n",
    "- **Performance Impact**: Collecting large amounts of data can be slow and may introduce significant overhead, especially when dealing with distributed computations where data needs to be transferred over the network.\n",
    "\n",
    "- **Not Scalable**: Use of `collect` undermines Spark's parallel processing capabilities. It forces all data to be handled by a single machine (the driver), negating the benefits of distributed processing for large-scale data analysis.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Use Sparingly**: Limit the use of `collect` to situations where you genuinely need to bring data to the driver for local processing or analysis.\n",
    "  \n",
    "- **Sampling**: If possible, sample a subset of data using techniques like `take` or `sample` before deciding to collect the entire dataset.\n",
    "\n",
    "- **Aggregation**: For summarizing results, prefer using Spark's built-in aggregation functions (`reduce`, `aggregate`, `fold`, etc.) instead of `collect` whenever feasible.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# Assuming 'rdd' is an RDD or 'df' is a DataFrame\n",
    "# Collecting data to the driver program (use with caution):\n",
    "collected_data = rdd.collect()\n",
    "\n",
    "# Process collected_data locally:\n",
    "for item in collected_data:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"Female\", 60000, \"HR\"),\n",
    "    (2, \"Bob\", \"Male\", 80000, \"Engineering\"),\n",
    "    (3, \"Charlie\", \"Male\", 75000, \"Marketing\"),\n",
    "    (4, \"Diana\", \"Female\", 90000, \"Finance\"),\n",
    "    (5, \"Eve\", \"Female\", 70000, \"Engineering\"),\n",
    "    (6, \"Frank\", \"Male\", 85000, \"HR\"),\n",
    "    (7, \"Gina\", \"Female\", 65000, \"Marketing\"),\n",
    "    (8, \"Henry\", \"Male\", 95000, \"Finance\"),\n",
    "    (9, \"Irene\", \"Female\", 72000, \"Engineering\"),\n",
    "    (10, \"John\", \"Male\", 78000, \"HR\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\"id\", \"name\", \"gender\",\"salary\",\"dept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+------+-----------+\n",
      "| id|   name|gender|salary|       dept|\n",
      "+---+-------+------+------+-----------+\n",
      "|  1|  Alice|Female| 60000|         HR|\n",
      "|  2|    Bob|  Male| 80000|Engineering|\n",
      "|  3|Charlie|  Male| 75000|  Marketing|\n",
      "|  4|  Diana|Female| 90000|    Finance|\n",
      "|  5|    Eve|Female| 70000|Engineering|\n",
      "|  6|  Frank|  Male| 85000|         HR|\n",
      "|  7|   Gina|Female| 65000|  Marketing|\n",
      "|  8|  Henry|  Male| 95000|    Finance|\n",
      "|  9|  Irene|Female| 72000|Engineering|\n",
      "| 10|   John|  Male| 78000|         HR|\n",
      "+---+-------+------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method collect in module pyspark.sql.dataframe:\n",
      "\n",
      "collect() -> List[pyspark.sql.types.Row] method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns all the records as a list of :class:`Row`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    list\n",
      "        List of rows.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame(\n",
      "    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      "    >>> df.collect()\n",
      "    [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Alice', gender='Female', salary=60000, dept='HR'),\n",
       " Row(id=2, name='Bob', gender='Male', salary=80000, dept='Engineering'),\n",
       " Row(id=3, name='Charlie', gender='Male', salary=75000, dept='Marketing'),\n",
       " Row(id=4, name='Diana', gender='Female', salary=90000, dept='Finance'),\n",
       " Row(id=5, name='Eve', gender='Female', salary=70000, dept='Engineering'),\n",
       " Row(id=6, name='Frank', gender='Male', salary=85000, dept='HR'),\n",
       " Row(id=7, name='Gina', gender='Female', salary=65000, dept='Marketing'),\n",
       " Row(id=8, name='Henry', gender='Male', salary=95000, dept='Finance'),\n",
       " Row(id=9, name='Irene', gender='Female', salary=72000, dept='Engineering'),\n",
       " Row(id=10, name='John', gender='Male', salary=78000, dept='HR')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=1, name='Alice', gender='Female', salary=60000, dept='HR')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
